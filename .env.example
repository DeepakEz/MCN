# ============================================================
# MCN Research-Grade Workstation Configuration
# ============================================================
# Copy this file to .env and customize for your hardware.
#
# Usage:
#   cp .env.example .env
#   # Edit .env with your settings
#   docker compose up --build
# ============================================================

# --- vLLM / LLM ---
# URL for the vLLM OpenAI-compatible API endpoint
MCN_VLLM_URL=http://vllm:8000/v1
# Model to load in vLLM (must fit in GPU VRAM)
# Good options for 16GB VRAM:
#   deepseek-ai/deepseek-coder-6.7b-instruct  (~13GB)
#   codellama/CodeLlama-13b-Instruct-hf        (~13GB)
#   Qwen/Qwen2.5-Coder-7B-Instruct            (~14GB)
MCN_VLLM_MODEL=deepseek-ai/deepseek-coder-6.7b-instruct
MCN_VLLM_API_KEY=EMPTY

# --- Redis ---
MCN_REDIS_URL=redis://redis:6379/0
MCN_USE_REDIS=true

# --- Tribes ---
MCN_NUM_TRIBES=3
MCN_TRIBE_TEMPERATURE=0.3
MCN_TRIBE_MAX_TOKENS=2048

# --- Sandbox ---
MCN_NUM_SANDBOXES=4
MCN_SANDBOX_IMAGE=mcn-sandbox:v0.1
# Set to /dev/shm for ramdisk-accelerated I/O (requires shm_size in compose)
MCN_SANDBOX_TMPDIR=/dev/shm
MCN_SANDBOX_MEM_LIMIT=256m
MCN_SANDBOX_TIMEOUT=10.0

# --- Council ---
MCN_BANDIT_ALPHA=1.5
MCN_DEEP_AUDIT_RATE=0.1

# --- Persistence ---
MCN_LOG_DIR=/results

# --- Phase 5: GNN Router (default: off, uses LinUCB) ---
MCN_USE_GNN_ROUTER=false
MCN_GNN_HIDDEN_DIM=32
MCN_GNN_LR=0.01
MCN_GNN_BUFFER_SIZE=64
MCN_GNN_BATCH_SIZE=8

# --- Phase 5: ChromaDB (default: off, uses in-memory PatchRegistry) ---
MCN_USE_CHROMADB=false
MCN_CHROMADB_COLLECTION=mcn_patches

# --- Phase 5: MLflow (default: off, no tracking) ---
MCN_USE_MLFLOW=false
MCN_MLFLOW_EXPERIMENT_NAME=mcn-experiments

# --- HuggingFace (required for gated models) ---
HF_TOKEN=<your-huggingface-token>
